{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid RL-Enhanced Cache Management - Colab Training\n",
    "\n",
    "Train a hybrid cache system where RL augments traditional heuristics (LRU/LFU) with eviction priority scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/haseebmalik18/name-undecided.git\n",
    "%cd name-undecided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "from agents.hybrid_environment import HybridCacheEnv\n",
    "from agents.priority_agent import PriorityAgent\n",
    "from workloads.generators import ZipfWorkload\n",
    "from metrics.tracker import MetricsTracker\n",
    "from visualization.plotter import plot_training_metrics, plot_comparison\n",
    "from cache.policies import LRUCache\n",
    "\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_CAPACITY = 100\n",
    "NUM_ITEMS = 1000\n",
    "EPISODES = 1000\n",
    "EPISODE_LENGTH = 1000\n",
    "ZIPF_ALPHA = 1.5\n",
    "BASE_POLICY = 'lru'\n",
    "RL_WEIGHT = 0.5\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hybrid Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload = ZipfWorkload(num_items=NUM_ITEMS, alpha=ZIPF_ALPHA, seed=42)\n",
    "\n",
    "env = HybridCacheEnv(\n",
    "    cache_capacity=CACHE_CAPACITY,\n",
    "    num_items=NUM_ITEMS,\n",
    "    workload_generator=workload,\n",
    "    episode_length=EPISODE_LENGTH,\n",
    "    base_policy=BASE_POLICY,\n",
    "    rl_weight=RL_WEIGHT\n",
    ")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "output_size = CACHE_CAPACITY\n",
    "\n",
    "agent = PriorityAgent(\n",
    "    state_size=state_size,\n",
    "    output_size=output_size,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "metrics = MetricsTracker()\n",
    "\n",
    "print(f\"State Size: {state_size}, Output Size: {output_size}\")\n",
    "print(f\"Base Policy: {BASE_POLICY.upper()}, RL Weight: {RL_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(EPISODES), desc=\"Training Hybrid\"):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(EPISODE_LENGTH):\n",
    "        action = agent.select_action(state, training=True)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        loss = agent.train()\n",
    "        \n",
    "        if loss is not None:\n",
    "            metrics.add_loss(loss)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    cache_metrics = env.cache.get_metrics()\n",
    "    metrics.add_episode(\n",
    "        total_reward,\n",
    "        cache_metrics['hit_rate'],\n",
    "        cache_metrics['avg_latency'],\n",
    "        cache_metrics['bandwidth_used']\n",
    "    )\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        stats = metrics.get_stats(window=100)\n",
    "        rl_influence = cache_metrics.get('rl_influence_rate', 0.0)\n",
    "        print(f\"\\nEpisode {episode + 1}/{EPISODES}\")\n",
    "        print(f\"  Hit Rate: {stats['mean_hit_rate']:.3f} ± {stats['std_hit_rate']:.3f}\")\n",
    "        print(f\"  Reward: {stats['mean_reward']:.2f}\")\n",
    "        print(f\"  RL Influence: {rl_influence:.2%}\")\n",
    "        print(f\"  Epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Pure LRU Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lru = LRUCache(capacity=CACHE_CAPACITY)\n",
    "lru_hit_rates = []\n",
    "\n",
    "workload_lru = ZipfWorkload(num_items=NUM_ITEMS, alpha=ZIPF_ALPHA, seed=42)\n",
    "\n",
    "for episode in tqdm(range(100), desc=\"LRU Eval\"):\n",
    "    lru.reset()\n",
    "    requests = workload_lru.generate(EPISODE_LENGTH)\n",
    "    \n",
    "    for req in requests:\n",
    "        lru.access(req)\n",
    "    \n",
    "    lru_hit_rates.append(lru.get_hit_rate())\n",
    "\n",
    "lru_mean = np.mean(lru_hit_rates)\n",
    "lru_std = np.std(lru_hit_rates)\n",
    "print(f\"Pure LRU Hit Rate: {lru_mean:.3f} ± {lru_std:.3f}\")\n",
    "\n",
    "hybrid_mean = np.mean(metrics.episode_hit_rates[-100:])\n",
    "print(f\"Hybrid Hit Rate: {hybrid_mean:.3f}\")\n",
    "print(f\"Improvement: {((hybrid_mean - lru_mean) / lru_mean * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Hybrid (LRU + RL)': {\n",
    "        'hit_rates': metrics.episode_hit_rates,\n",
    "        'rewards': metrics.episode_rewards\n",
    "    },\n",
    "    'LRU Only': {\n",
    "        'hit_rates': lru_hit_rates + [lru_mean] * (len(metrics.episode_hit_rates) - len(lru_hit_rates)),\n",
    "        'rewards': [0] * len(metrics.episode_rewards)\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Influence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = env.cache.get_metrics()\n",
    "print(f\"Total Evictions: {final_metrics['evictions']}\")\n",
    "print(f\"RL Influenced Evictions: {final_metrics['rl_influenced_evictions']}\")\n",
    "print(f\"RL Influence Rate: {final_metrics['rl_influence_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('hybrid_agent_colab.pth')\n",
    "metrics.save('metrics_hybrid_colab.json')\n",
    "print(\"Model and metrics saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('hybrid_agent_colab.pth')\n",
    "files.download('metrics_hybrid_colab.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
