{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Hybrid RL-Enhanced Cache with Dynamic Workloads\n",
    "Training on temporal patterns to demonstrate RL value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple, Iterator\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRUCache:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.cache = OrderedDict()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "\n",
    "    def access(self, item: int) -> bool:\n",
    "        if item in self.cache:\n",
    "            self.cache.move_to_end(item)\n",
    "            self.hits += 1\n",
    "            return True\n",
    "        else:\n",
    "            self.misses += 1\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                self.cache.popitem(last=False)\n",
    "            self.cache[item] = True\n",
    "            return False\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        total = self.hits + self.misses\n",
    "        return {\n",
    "            'hit_rate': self.hits / total if total > 0 else 0,\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'total_accesses': total\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.cache.clear()\n",
    "        self.hits = 0\n",
    "        self.misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCache:\n",
    "    def __init__(self, capacity: int, base_policy: str = 'lru', rl_weight: float = 0.5,\n",
    "                 miss_penalty: float = 10.0, hit_latency: float = 1.0):\n",
    "        self.capacity = capacity\n",
    "        self.base_policy = base_policy\n",
    "        self.rl_weight = rl_weight\n",
    "        self.miss_penalty = miss_penalty\n",
    "        self.hit_latency = hit_latency\n",
    "\n",
    "        self.cache = OrderedDict()\n",
    "        self.access_frequency = defaultdict(int)\n",
    "        self.last_access_time = {}\n",
    "        self.current_time = 0\n",
    "\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.evictions = 0\n",
    "        self.rl_influenced_evictions = 0\n",
    "\n",
    "    def access(self, item: int) -> Tuple[bool, float]:\n",
    "        self.current_time += 1\n",
    "        self.last_access_time[item] = self.current_time\n",
    "        self.access_frequency[item] += 1\n",
    "\n",
    "        if item in self.cache:\n",
    "            self.cache.move_to_end(item)\n",
    "            self.hits += 1\n",
    "            return True, self.hit_latency\n",
    "        else:\n",
    "            self.misses += 1\n",
    "            return False, self.miss_penalty\n",
    "\n",
    "    def admit(self, item: int, rl_scores: Optional[Dict[int, float]] = None) -> Optional[int]:\n",
    "        evicted = None\n",
    "        if len(self.cache) >= self.capacity:\n",
    "            if rl_scores and self.rl_weight > 0:\n",
    "                evicted = self._evict_with_rl(rl_scores)\n",
    "            else:\n",
    "                evicted = self._base_eviction()\n",
    "\n",
    "        self.cache[item] = True\n",
    "        self.cache.move_to_end(item)\n",
    "        return evicted\n",
    "\n",
    "    def _evict_with_rl(self, rl_scores: Dict[int, float]) -> int:\n",
    "        base_scores = self._get_base_scores()\n",
    "        combined_scores = {}\n",
    "\n",
    "        for item in self.cache.keys():\n",
    "            base_score = base_scores.get(item, 0.5)\n",
    "            rl_score = rl_scores.get(item, 0.5)\n",
    "            combined_scores[item] = (1 - self.rl_weight) * base_score + self.rl_weight * rl_score\n",
    "\n",
    "        base_victim = max(base_scores.items(), key=lambda x: x[1])[0]\n",
    "        rl_victim = max(combined_scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "        if base_victim != rl_victim:\n",
    "            self.rl_influenced_evictions += 1\n",
    "\n",
    "        self.evictions += 1\n",
    "        del self.cache[rl_victim]\n",
    "        return rl_victim\n",
    "\n",
    "    def _base_eviction(self) -> int:\n",
    "        base_scores = self._get_base_scores()\n",
    "        victim = max(base_scores.items(), key=lambda x: x[1])[0]\n",
    "        self.evictions += 1\n",
    "        del self.cache[victim]\n",
    "        return victim\n",
    "\n",
    "    def _get_base_scores(self) -> Dict[int, float]:\n",
    "        scores = {}\n",
    "        cache_items = list(self.cache.keys())\n",
    "\n",
    "        if self.base_policy == 'lru':\n",
    "            for idx, item in enumerate(cache_items):\n",
    "                scores[item] = 1.0 - (idx / len(cache_items))\n",
    "        elif self.base_policy == 'lfu':\n",
    "            max_freq = max([self.access_frequency[item] for item in cache_items], default=1)\n",
    "            for item in cache_items:\n",
    "                freq = self.access_frequency[item]\n",
    "                scores[item] = 1.0 - (freq / max_freq)\n",
    "        else:\n",
    "            for item in cache_items:\n",
    "                scores[item] = 0.5\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        total = self.hits + self.misses\n",
    "        return {\n",
    "            'hit_rate': self.hits / total if total > 0 else 0,\n",
    "            'miss_rate': self.misses / total if total > 0 else 0,\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'evictions': self.evictions,\n",
    "            'rl_influence_rate': self.rl_influenced_evictions / self.evictions if self.evictions > 0 else 0,\n",
    "            'total_accesses': total,\n",
    "            'avg_latency': (self.hits * self.hit_latency + self.misses * self.miss_penalty) / total if total > 0 else 0\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.cache.clear()\n",
    "        self.access_frequency.clear()\n",
    "        self.last_access_time.clear()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.evictions = 0\n",
    "        self.rl_influenced_evictions = 0\n",
    "        self.current_time = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cache)\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalShiftWorkload:\n",
    "    def __init__(self, num_items: int, phase_length: int = 200, num_popular_sets: int = 3,\n",
    "                 popular_set_size: int = 20, alpha: float = 1.5, seed: Optional[int] = None):\n",
    "        self.num_items = num_items\n",
    "        self.phase_length = phase_length\n",
    "        self.num_popular_sets = num_popular_sets\n",
    "        self.popular_set_size = popular_set_size\n",
    "        self.alpha = alpha\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.request_count = 0\n",
    "\n",
    "        self.popular_sets = []\n",
    "        items = list(range(num_items))\n",
    "        self.rng.shuffle(items)\n",
    "        for i in range(num_popular_sets):\n",
    "            start = i * popular_set_size\n",
    "            end = start + popular_set_size\n",
    "            self.popular_sets.append(items[start:end])\n",
    "\n",
    "    def _current_phase(self) -> int:\n",
    "        return (self.request_count // self.phase_length) % self.num_popular_sets\n",
    "\n",
    "    def generate(self, num_requests: int) -> List[int]:\n",
    "        requests = []\n",
    "        for _ in range(num_requests):\n",
    "            phase = self._current_phase()\n",
    "            popular_items = self.popular_sets[phase]\n",
    "\n",
    "            if self.rng.random() < 0.8:\n",
    "                idx = min(self.rng.zipf(self.alpha), len(popular_items)) - 1\n",
    "                item = popular_items[idx % len(popular_items)]\n",
    "            else:\n",
    "                item = self.rng.randint(0, self.num_items)\n",
    "\n",
    "            requests.append(item)\n",
    "            self.request_count += 1\n",
    "\n",
    "        return requests\n",
    "\n",
    "    def reset(self):\n",
    "        self.request_count = 0\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        while True:\n",
    "            phase = self._current_phase()\n",
    "            popular_items = self.popular_sets[phase]\n",
    "\n",
    "            if self.rng.random() < 0.8:\n",
    "                idx = min(self.rng.zipf(self.alpha), len(popular_items)) - 1\n",
    "                item = popular_items[idx % len(popular_items)]\n",
    "            else:\n",
    "                item = self.rng.randint(0, self.num_items)\n",
    "\n",
    "            self.request_count += 1\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularitySpikeWorkload:\n",
    "    def __init__(self, num_items: int, alpha: float = 1.5, spike_probability: float = 0.01,\n",
    "                 spike_duration: int = 50, spike_intensity: float = 0.9, seed: Optional[int] = None):\n",
    "        self.num_items = num_items\n",
    "        self.alpha = alpha\n",
    "        self.spike_probability = spike_probability\n",
    "        self.spike_duration = spike_duration\n",
    "        self.spike_intensity = spike_intensity\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "        self.current_spike_item = None\n",
    "        self.spike_remaining = 0\n",
    "        self.request_count = 0\n",
    "\n",
    "    def generate(self, num_requests: int) -> List[int]:\n",
    "        requests = []\n",
    "        for _ in range(num_requests):\n",
    "            if self.spike_remaining == 0 and self.rng.random() < self.spike_probability:\n",
    "                self.current_spike_item = self.rng.randint(0, self.num_items)\n",
    "                self.spike_remaining = self.spike_duration\n",
    "\n",
    "            if self.spike_remaining > 0:\n",
    "                if self.rng.random() < self.spike_intensity:\n",
    "                    item = self.current_spike_item\n",
    "                else:\n",
    "                    item = self.rng.zipf(self.alpha) % self.num_items\n",
    "                self.spike_remaining -= 1\n",
    "            else:\n",
    "                item = self.rng.zipf(self.alpha) % self.num_items\n",
    "\n",
    "            requests.append(item)\n",
    "            self.request_count += 1\n",
    "\n",
    "        return requests\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_spike_item = None\n",
    "        self.spike_remaining = 0\n",
    "        self.request_count = 0\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        while True:\n",
    "            if self.spike_remaining == 0 and self.rng.random() < self.spike_probability:\n",
    "                self.current_spike_item = self.rng.randint(0, self.num_items)\n",
    "                self.spike_remaining = self.spike_duration\n",
    "\n",
    "            if self.spike_remaining > 0:\n",
    "                if self.rng.random() < self.spike_intensity:\n",
    "                    item = self.current_spike_item\n",
    "                else:\n",
    "                    item = self.rng.zipf(self.alpha) % self.num_items\n",
    "                self.spike_remaining -= 1\n",
    "            else:\n",
    "                item = self.rng.zipf(self.alpha) % self.num_items\n",
    "\n",
    "            self.request_count += 1\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeOfDayWorkload:\n",
    "    def __init__(self, num_items: int, cycle_length: int = 500, num_cycles: int = 4,\n",
    "                 phase_overlap: float = 0.1, seed: Optional[int] = None):\n",
    "        self.num_items = num_items\n",
    "        self.cycle_length = cycle_length\n",
    "        self.num_cycles = num_cycles\n",
    "        self.phase_overlap = phase_overlap\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.request_count = 0\n",
    "\n",
    "        self.cycle_items = []\n",
    "        items_per_cycle = num_items // num_cycles\n",
    "        for i in range(num_cycles):\n",
    "            start = i * items_per_cycle\n",
    "            end = start + items_per_cycle\n",
    "            self.cycle_items.append(list(range(start, end)))\n",
    "\n",
    "    def generate(self, num_requests: int) -> List[int]:\n",
    "        requests = []\n",
    "        for _ in range(num_requests):\n",
    "            position = (self.request_count % self.cycle_length) / self.cycle_length\n",
    "            phase = int(position * self.num_cycles)\n",
    "\n",
    "            main_items = self.cycle_items[phase]\n",
    "\n",
    "            if self.rng.random() < self.phase_overlap:\n",
    "                adjacent_phase = (phase + 1) % self.num_cycles\n",
    "                main_items = main_items + self.cycle_items[adjacent_phase]\n",
    "\n",
    "            if len(main_items) > 0:\n",
    "                idx = min(self.rng.zipf(1.5), len(main_items)) - 1\n",
    "                item = main_items[idx % len(main_items)]\n",
    "            else:\n",
    "                item = self.rng.randint(0, self.num_items)\n",
    "\n",
    "            requests.append(item)\n",
    "            self.request_count += 1\n",
    "\n",
    "        return requests\n",
    "\n",
    "    def reset(self):\n",
    "        self.request_count = 0\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        while True:\n",
    "            position = (self.request_count % self.cycle_length) / self.cycle_length\n",
    "            phase = int(position * self.num_cycles)\n",
    "\n",
    "            main_items = self.cycle_items[phase]\n",
    "\n",
    "            if self.rng.random() < self.phase_overlap:\n",
    "                adjacent_phase = (phase + 1) % self.num_cycles\n",
    "                main_items = main_items + self.cycle_items[adjacent_phase]\n",
    "\n",
    "            if len(main_items) > 0:\n",
    "                idx = min(self.rng.zipf(1.5), len(main_items)) - 1\n",
    "                item = main_items[idx % len(main_items)]\n",
    "            else:\n",
    "                item = self.rng.randint(0, self.num_items)\n",
    "\n",
    "            self.request_count += 1\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCacheEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, cache_capacity: int, num_items: int, workload_generator,\n",
    "                 episode_length: int = 1000, state_size: int = 30, base_policy: str = 'lru',\n",
    "                 rl_weight: float = 0.5, alpha: float = 1.0, beta: float = 0.1, gamma: float = 0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cache = HybridCache(\n",
    "            capacity=cache_capacity,\n",
    "            base_policy=base_policy,\n",
    "            rl_weight=rl_weight,\n",
    "            miss_penalty=10.0,\n",
    "            hit_latency=1.0\n",
    "        )\n",
    "        self.num_items = num_items\n",
    "        self.workload = workload_generator\n",
    "        self.episode_length = episode_length\n",
    "        self.state_size = state_size\n",
    "        self.base_policy = base_policy\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.current_request = None\n",
    "\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(cache_capacity,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(state_size,), dtype=np.float32)\n",
    "\n",
    "        self.request_history = []\n",
    "        self.item_to_cache_idx = {}\n",
    "        self.item_last_access_time = {}\n",
    "        self.item_access_trend = {}\n",
    "        self.item_recent_frequency = {}\n",
    "        self.item_historical_frequency = {}\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None) -> Tuple[np.ndarray, Dict]:\n",
    "        super().reset(seed=seed)\n",
    "        self.cache.reset()\n",
    "        self.current_step = 0\n",
    "        self.request_history = []\n",
    "        self.item_to_cache_idx = {}\n",
    "        self.item_last_access_time = {}\n",
    "        self.item_access_trend = {}\n",
    "        self.item_recent_frequency = {}\n",
    "        self.item_historical_frequency = {}\n",
    "        self.current_request = next(iter(self.workload))\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        is_hit, latency = self.cache.access(self.current_request)\n",
    "        rl_eviction_scores = self._action_to_eviction_scores(action)\n",
    "\n",
    "        evicted = None\n",
    "        if not is_hit:\n",
    "            evicted = self.cache.admit(self.current_request, rl_eviction_scores)\n",
    "\n",
    "        hit_reward = self.alpha if is_hit else -self.alpha\n",
    "        latency_penalty = -self.beta * latency / self.cache.miss_penalty\n",
    "        bandwidth_penalty = 0 if is_hit else -self.gamma\n",
    "        reward = hit_reward + latency_penalty + bandwidth_penalty\n",
    "\n",
    "        if evicted is not None:\n",
    "            future_access_penalty = 0.0\n",
    "            if evicted in self.request_history[-10:]:\n",
    "                future_access_penalty = -0.2\n",
    "            trend = self.item_access_trend.get(evicted, 0.0)\n",
    "            if trend > 0.5:\n",
    "                reward += -0.3\n",
    "            reward += future_access_penalty\n",
    "\n",
    "        self._update_temporal_features(self.current_request)\n",
    "        self.request_history.append(self.current_request)\n",
    "        if len(self.request_history) > 100:\n",
    "            self.request_history.pop(0)\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.episode_length\n",
    "        truncated = False\n",
    "        self.current_request = next(iter(self.workload))\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        info = {\n",
    "            'hit_rate': self.cache.get_metrics()['hit_rate'],\n",
    "            'cache_size': len(self.cache),\n",
    "            'evicted': evicted,\n",
    "            'rl_influenced': self.cache.rl_influenced_evictions\n",
    "        }\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _action_to_eviction_scores(self, action: np.ndarray) -> Dict[Any, float]:\n",
    "        scores = {}\n",
    "        cache_items = list(self.cache.cache.keys())\n",
    "        self.item_to_cache_idx = {}\n",
    "        for idx, item in enumerate(cache_items):\n",
    "            self.item_to_cache_idx[item] = idx\n",
    "        for item in cache_items:\n",
    "            idx = self.item_to_cache_idx.get(item, 0)\n",
    "            if idx < len(action):\n",
    "                scores[item] = float(action[idx])\n",
    "            else:\n",
    "                scores[item] = 0.5\n",
    "        return scores\n",
    "\n",
    "    def _update_temporal_features(self, item: int):\n",
    "        self.item_last_access_time[item] = self.current_step\n",
    "        self.item_recent_frequency[item] = self.item_recent_frequency.get(item, 0) + 1\n",
    "        self.item_historical_frequency[item] = self.item_historical_frequency.get(item, 0) + 1\n",
    "\n",
    "        if self.current_step % 100 == 0:\n",
    "            for it in list(self.item_recent_frequency.keys()):\n",
    "                recent_freq = self.item_recent_frequency.get(it, 0)\n",
    "                hist_freq = self.item_historical_frequency.get(it, 1)\n",
    "                trend = recent_freq / max(hist_freq / (self.current_step / 100 + 1), 1)\n",
    "                self.item_access_trend[it] = min(trend, 1.0)\n",
    "                self.item_recent_frequency[it] = 0\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        state = np.zeros(self.state_size, dtype=np.float32)\n",
    "        state[0] = len(self.cache) / self.cache.capacity\n",
    "        state[1] = self.current_request / self.num_items\n",
    "        freq = self.cache.access_frequency.get(self.current_request, 0)\n",
    "        state[2] = min(freq / 10.0, 1.0)\n",
    "        if self.current_request in self.cache:\n",
    "            state[3] = 1.0\n",
    "        recent_unique = len(set(self.request_history[-20:]))\n",
    "        state[4] = recent_unique / 20.0 if self.request_history else 0.0\n",
    "        metrics = self.cache.get_metrics()\n",
    "        state[5] = metrics['hit_rate']\n",
    "        if len(self.request_history) >= 5:\n",
    "            recent_requests = self.request_history[-5:]\n",
    "            state[6] = 1.0 if self.current_request in recent_requests else 0.0\n",
    "        cache_items = list(self.cache.cache.keys())\n",
    "        for i, item in enumerate(cache_items[:10]):\n",
    "            freq = self.cache.access_frequency.get(item, 0)\n",
    "            state[7 + i] = min(freq / 10.0, 1.0)\n",
    "        state[17] = metrics.get('rl_influence_rate', 0.0)\n",
    "        if len(self.request_history) >= 3:\n",
    "            pattern = [self.request_history[-3], self.request_history[-2], self.request_history[-1]]\n",
    "            state[18] = 1.0 if self.current_request in pattern else 0.0\n",
    "        if self.current_request in self.item_last_access_time:\n",
    "            time_since_access = self.current_step - self.item_last_access_time[self.current_request]\n",
    "            state[19] = min(time_since_access / 100.0, 1.0)\n",
    "        else:\n",
    "            state[19] = 1.0\n",
    "        state[20] = self.item_access_trend.get(self.current_request, 0.0)\n",
    "        for idx, item in enumerate(cache_items[:5]):\n",
    "            if item in self.item_last_access_time:\n",
    "                recency = self.current_step - self.item_last_access_time[item]\n",
    "                state[21 + idx] = min(recency / 100.0, 1.0)\n",
    "            else:\n",
    "                state[21 + idx] = 1.0\n",
    "        for idx, item in enumerate(cache_items[:4]):\n",
    "            state[26 + idx] = self.item_access_trend.get(item, 0.0)\n",
    "        return state\n",
    "\n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityNetwork(torch.nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int, hidden_size: int = 128):\n",
    "        super(PriorityNetwork, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in batch])\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class PriorityAgent:\n",
    "    def __init__(self, state_size: int, action_size: int, lr: float = 0.001, gamma: float = 0.99,\n",
    "                 epsilon: float = 1.0, epsilon_min: float = 0.01, epsilon_decay: float = 0.995,\n",
    "                 device: str = 'cpu', batch_size: int = 64, buffer_capacity: int = 10000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.policy_net = PriorityNetwork(state_size, action_size).to(device)\n",
    "        self.target_net = PriorityNetwork(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.rand(self.action_size).astype(np.float32)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                priorities = self.policy_net(state_tensor)\n",
    "                return priorities.cpu().numpy()[0]\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return 0.0\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.FloatTensor(np.array(actions)).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        current_priorities = self.policy_net(states)\n",
    "        next_priorities = self.target_net(next_states).detach()\n",
    "\n",
    "        target_priorities = rewards + (1 - dones) * self.gamma * next_priorities.mean(dim=1, keepdim=True)\n",
    "        loss = torch.nn.functional.mse_loss(current_priorities.mean(dim=1, keepdim=True), target_priorities)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, path: str):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKLOAD_TYPE = 'temporal_shift'\n",
    "CACHE_SIZE = 100\n",
    "NUM_ITEMS = 1000\n",
    "EPISODES = 1000\n",
    "STEPS_PER_EPISODE = 1000\n",
    "RL_WEIGHT = 0.5\n",
    "BASE_POLICY = 'lru'\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workload and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WORKLOAD_TYPE == 'temporal_shift':\n",
    "    workload = TemporalShiftWorkload(num_items=NUM_ITEMS, seed=42)\n",
    "elif WORKLOAD_TYPE == 'popularity_spike':\n",
    "    workload = PopularitySpikeWorkload(num_items=NUM_ITEMS, seed=42)\n",
    "elif WORKLOAD_TYPE == 'time_of_day':\n",
    "    workload = TimeOfDayWorkload(num_items=NUM_ITEMS, seed=42)\n",
    "else:\n",
    "    workload = TemporalShiftWorkload(num_items=NUM_ITEMS, seed=42)\n",
    "\n",
    "env = HybridCacheEnv(\n",
    "    cache_capacity=CACHE_SIZE,\n",
    "    num_items=NUM_ITEMS,\n",
    "    workload_generator=workload,\n",
    "    episode_length=STEPS_PER_EPISODE,\n",
    "    state_size=30,\n",
    "    base_policy=BASE_POLICY,\n",
    "    rl_weight=RL_WEIGHT\n",
    ")\n",
    "\n",
    "agent = PriorityAgent(\n",
    "    state_size=30,\n",
    "    action_size=CACHE_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'episode_rewards': [],\n",
    "    'episode_hit_rates': [],\n",
    "    'episode_rl_influence': [],\n",
    "    'episode_avg_latency': [],\n",
    "    'best_hit_rate': 0.0\n",
    "}\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    episode_hit_rates = []\n",
    "\n",
    "    for step in range(STEPS_PER_EPISODE):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        agent.store_transition(state, action, reward, next_state, done or truncated)\n",
    "        agent.train()\n",
    "        total_reward += reward\n",
    "        episode_hit_rates.append(info['hit_rate'])\n",
    "        state = next_state\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    cache_metrics = env.cache.get_metrics()\n",
    "    avg_hit_rate = np.mean(episode_hit_rates)\n",
    "    metrics['episode_rewards'].append(total_reward)\n",
    "    metrics['episode_hit_rates'].append(avg_hit_rate)\n",
    "    metrics['episode_rl_influence'].append(cache_metrics.get('rl_influence_rate', 0.0))\n",
    "    metrics['episode_avg_latency'].append(cache_metrics.get('avg_latency', 0.0))\n",
    "\n",
    "    if avg_hit_rate > metrics['best_hit_rate']:\n",
    "        metrics['best_hit_rate'] = avg_hit_rate\n",
    "\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Ep {episode + 1}/{EPISODES} | Reward: {total_reward:.2f} | \"\n",
    "              f\"Hit Rate: {avg_hit_rate:.4f} | RL Influence: {cache_metrics.get('rl_influence_rate', 0.0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_workload = TemporalShiftWorkload(num_items=NUM_ITEMS, seed=42) if WORKLOAD_TYPE == 'temporal_shift' else workload\n",
    "baseline_cache = LRUCache(CACHE_SIZE)\n",
    "baseline_requests = baseline_workload.generate(STEPS_PER_EPISODE)\n",
    "\n",
    "for req in baseline_requests:\n",
    "    baseline_cache.access(req)\n",
    "\n",
    "baseline_metrics = baseline_cache.get_metrics()\n",
    "baseline_hit_rate = baseline_metrics['hit_rate']\n",
    "\n",
    "improvement = ((metrics['best_hit_rate'] - baseline_hit_rate) / baseline_hit_rate) * 100\n",
    "\n",
    "print(f\"\\nBaseline LRU Hit Rate: {baseline_hit_rate:.4f}\")\n",
    "print(f\"RL-Enhanced Hit Rate: {metrics['best_hit_rate']:.4f}\")\n",
    "print(f\"Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(metrics['episode_hit_rates'], alpha=0.6)\n",
    "axes[0, 0].axhline(y=baseline_hit_rate, color='r', linestyle='--', label='Baseline LRU')\n",
    "axes[0, 0].set_title('Hit Rate Over Training')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Hit Rate')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(metrics['episode_rewards'], alpha=0.6)\n",
    "axes[0, 1].set_title('Episode Rewards')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Total Reward')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(metrics['episode_rl_influence'], alpha=0.6)\n",
    "axes[1, 0].set_title('RL Influence Rate')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('RL Influence Rate')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "improvement_per_ep = [(hr - baseline_hit_rate) / baseline_hit_rate * 100 for hr in metrics['episode_hit_rates']]\n",
    "axes[1, 1].plot(improvement_per_ep, alpha=0.6)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-')\n",
    "axes[1, 1].set_title('Improvement Over Baseline (%)')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Improvement (%)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{WORKLOAD_TYPE}_training_results.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(f'dynamic_agent_{WORKLOAD_TYPE}_colab.pth')\n",
    "with open(f'metrics_{WORKLOAD_TYPE}_colab.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "from google.colab import files\n",
    "files.download(f'dynamic_agent_{WORKLOAD_TYPE}_colab.pth')\n",
    "files.download(f'metrics_{WORKLOAD_TYPE}_colab.json')\n",
    "files.download(f'{WORKLOAD_TYPE}_training_results.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
